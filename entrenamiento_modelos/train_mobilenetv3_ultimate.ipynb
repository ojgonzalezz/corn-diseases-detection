{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåΩ Entrenamiento MobileNetV3 - ARQUITECTURA 100/100\n",
    "\n",
    "**Objetivo: >85% Accuracy + >80% Recall**\n",
    "\n",
    "## üéØ Optimizaciones Clave:\n",
    "1. ‚úÖ Arquitectura 384‚Üí192 (m√°s capacidad que 256‚Üí128)\n",
    "2. ‚úÖ Batch size 32 (mejor generalizaci√≥n)\n",
    "3. ‚úÖ 60 √©pocas iniciales (m√°s tiempo para converger)\n",
    "4. ‚úÖ Fine-tuning ULTRA CONSERVADOR (10 capas, LR=0.000025)\n",
    "5. ‚úÖ Cosine Decay LR Schedule\n",
    "6. ‚úÖ Callbacks optimizados para recall\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß BLOQUE 1: Setup y Verificaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Montar Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 1.2 Clonar repositorio\n",
    "!git clone -b main https://github.com/ojgonzalezz/corn-diseases-detection.git\n",
    "%cd corn-diseases-detection/entrenamiento_modelos\n",
    "\n",
    "# 1.3 Instalar dependencias\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# 1.4 Crear directorios necesarios en Drive\n",
    "!mkdir -p /content/drive/MyDrive/corn-diseases-detection/models\n",
    "!mkdir -p /content/drive/MyDrive/corn-diseases-detection/logs\n",
    "!mkdir -p /content/drive/MyDrive/corn-diseases-detection/mlruns\n",
    "\n",
    "print(\"\\n‚úÖ Setup completado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è BLOQUE 2: Configuraci√≥n OPTIMIZADA y Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Importar configuraci√≥n base\n",
    "from config import *\n",
    "from utils import setup_gpu\n",
    "\n",
    "# ==================== CONFIGURACI√ìN OPTIMIZADA ====================\n",
    "# Override de hiperpar√°metros para arquitectura 100/100\n",
    "BATCH_SIZE = 32  # Mejor generalizaci√≥n que 64\n",
    "EPOCHS = 60  # Aumentado de 40 para mejor convergencia\n",
    "LEARNING_RATE = 0.001  # LR inicial\n",
    "EARLY_STOPPING_PATIENCE = 20  # M√°s paciencia\n",
    "\n",
    "# Configurar GPU\n",
    "setup_gpu(GPU_MEMORY_LIMIT)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CONFIGURACI√ìN ARQUITECTURA 100/100\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"√âpocas Iniciales: {EPOCHS}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Early Stopping Patience: {EARLY_STOPPING_PATIENCE}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear generadores de datos\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "print(\"Creando generadores de datos...\\n\")\n",
    "\n",
    "# Solo rescale (augmentation ya aplicado en preprocessing)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=VAL_SPLIT + TEST_SPLIT\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=VAL_SPLIT + TEST_SPLIT\n",
    ")\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "val_gen = val_datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "test_gen = val_datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"üìä Dataset:\")\n",
    "print(f\"  Training:   {train_gen.samples} im√°genes ({train_gen.samples // BATCH_SIZE} batches)\")\n",
    "print(f\"  Validation: {val_gen.samples} im√°genes ({val_gen.samples // BATCH_SIZE} batches)\")\n",
    "print(f\"  Test:       {test_gen.samples} im√°genes ({test_gen.samples // BATCH_SIZE} batches)\")\n",
    "\n",
    "# Calcular class weights para maximizar recall\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_gen.classes),\n",
    "    y=train_gen.classes\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(f\"\\n‚öñÔ∏è Class weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelo con ARQUITECTURA 100/100\n",
    "def create_ultimate_model(num_classes, image_size, initial_learning_rate, steps_per_epoch):\n",
    "    \"\"\"\n",
    "    Arquitectura 100/100 optimizada para >85% accuracy y >80% recall\n",
    "    \n",
    "    Mejoras clave:\n",
    "    - Dense(384) ‚Üí Dense(192): M√°s capacidad que 256‚Üí128\n",
    "    - Dropout(0.4, 0.35): Mayor regularizaci√≥n\n",
    "    - Cosine Decay LR: Mejor convergencia\n",
    "    - L2 regularization: 0.001\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cargar base preentrenada\n",
    "    base_model = MobileNetV3Large(\n",
    "        input_shape=(*image_size, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # Congelar capas base inicialmente\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # ARQUITECTURA 100/100: 384 ‚Üí 192\n",
    "    inputs = tf.keras.Input(shape=(*image_size, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Primera capa densa: 384 neuronas (50% m√°s que 256)\n",
    "    x = Dense(384, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "    x = Dropout(0.4)(x)  # Dropout alto para mejor generalizaci√≥n\n",
    "    \n",
    "    # Segunda capa densa: 192 neuronas (50% m√°s que 128)\n",
    "    x = Dense(192, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "    x = Dropout(0.35)(x)\n",
    "    \n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    # Cosine Decay Learning Rate Schedule\n",
    "    lr_schedule = CosineDecay(\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        decay_steps=steps_per_epoch * 60,  # 60 √©pocas\n",
    "        alpha=0.1  # LR final = 10% del inicial\n",
    "    )\n",
    "    \n",
    "    # Compilar con LR schedule\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr_schedule),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear modelo\n",
    "print(\"\\nüèóÔ∏è Creando modelo con arquitectura 100/100...\\n\")\n",
    "steps_per_epoch = train_gen.samples // BATCH_SIZE\n",
    "\n",
    "model = create_ultimate_model(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    initial_learning_rate=LEARNING_RATE,\n",
    "    steps_per_epoch=steps_per_epoch\n",
    ")\n",
    "\n",
    "print(f\"üìê Total par√°metros: {model.count_params():,}\")\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "print(f\"üìê Par√°metros entrenables: {trainable_params:,}\")\n",
    "print(f\"üìê Ratio datos/params: {train_gen.samples / trainable_params:.2f}\")\n",
    "print(\"\\n‚úÖ Modelo creado con arquitectura 100/100!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ BLOQUE 3: Entrenamiento Inicial (60 √©pocas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks para entrenamiento inicial\n",
    "# NOTA: NO usar ReduceLROnPlateau con CosineDecay (incompatible)\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=EARLY_STOPPING_PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        mode='max'\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        str(MODELS_DIR / 'mobilenetv3_best.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "        mode='max'\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üöÄ INICIANDO ENTRENAMIENTO INICIAL (60 √âPOCAS)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "print(\"Objetivo: >85% accuracy, >80% recall\")\n",
    "print(\"Arquitectura: 384‚Üí192 (100/100)\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: Cosine Decay desde {LEARNING_RATE}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_gen,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "best_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ ENTRENAMIENTO INICIAL COMPLETADO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"‚è±Ô∏è  Tiempo: {training_time/60:.2f} minutos\")\n",
    "print(f\"üìä Mejor Val Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%) en √©poca {best_epoch}\")\n",
    "print(f\"üìä Train Accuracy final: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ BLOQUE 4: Fine-tuning ULTRA CONSERVADOR (10 √©pocas)\n",
    "\n",
    "**IMPORTANTE:** Solo ejecuta este bloque si:\n",
    "- Val Accuracy < 85% despu√©s del entrenamiento inicial\n",
    "- El modelo muestra potencial de mejora (no hay overfitting severo)\n",
    "\n",
    "Si ya tienes >85% accuracy, **SALTA este bloque** y ve directo al Bloque 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si vale la pena hacer fine-tuning\n",
    "if best_val_acc >= 0.85:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üéâ ¬°OBJETIVO ALCANZADO SIN FINE-TUNING!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Val Accuracy: {best_val_acc:.4f} (>85%)\")\n",
    "    print(\"No es necesario ejecutar fine-tuning.\")\n",
    "    print(\"Ve directo al BLOQUE 5 para evaluaci√≥n.\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "else:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üéØ INICIANDO FINE-TUNING ULTRA CONSERVADOR\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Val Accuracy actual: {best_val_acc:.4f} (<85%)\")\n",
    "    print(\"Descongelando SOLO 10 capas finales...\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Descongelar SOLO las √∫ltimas 10 capas (ultra conservador)\n",
    "    base_model = model.layers[1]\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    for layer in base_model.layers[:-10]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    trainable_layers = sum([1 for layer in base_model.layers if layer.trainable])\n",
    "    print(f\"üîì Capas descongeladas: {trainable_layers} de {len(base_model.layers)}\\n\")\n",
    "    \n",
    "    # Recompilar con LR ULTRA BAJO: 0.000025 (2.5% del LR inicial)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE * 0.025),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks para fine-tuning ultra conservador\n",
    "    # NOTA: NO usar ReduceLROnPlateau con learning rate fijo bajo\n",
    "    finetune_callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=5,  # Parar r√°pido si empeora\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            mode='max'\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            str(MODELS_DIR / 'mobilenetv3_best.keras'),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1,\n",
    "            mode='max'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Fine-tuning con SOLO 10 √©pocas\n",
    "    history_finetune = model.fit(\n",
    "        train_gen,\n",
    "        epochs=10,\n",
    "        validation_data=val_gen,\n",
    "        callbacks=finetune_callbacks,\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Combinar historiales\n",
    "    for key in history.history:\n",
    "        history.history[key].extend(history_finetune.history[key])\n",
    "    \n",
    "    finetune_time = time.time() - start_time - training_time\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    best_val_acc_finetune = max(history_finetune.history['val_accuracy'])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"‚úÖ FINE-TUNING COMPLETADO\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"‚è±Ô∏è  Tiempo fine-tuning: {finetune_time/60:.2f} minutos\")\n",
    "    print(f\"‚è±Ô∏è  Tiempo total: {total_time/60:.2f} minutos\")\n",
    "    print(f\"üìä Mejor Val Accuracy (fine-tuning): {best_val_acc_finetune:.4f}\")\n",
    "    \n",
    "    if best_val_acc_finetune > best_val_acc:\n",
    "        print(f\"üìà ¬°Mejora lograda! {best_val_acc:.4f} ‚Üí {best_val_acc_finetune:.4f}\")\n",
    "    else:\n",
    "        print(f\"üìâ No hubo mejora. Mejor resultado sigue siendo: {best_val_acc:.4f}\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä BLOQUE 5: Evaluaci√≥n Completa y Guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import json\n",
    "from datetime import datetime\n",
    "from utils import evaluate_model, plot_training_history, plot_confusion_matrix, save_training_log\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìä EVALUACI√ìN EN TEST SET\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Evaluar modelo en test set\n",
    "evaluation_results = evaluate_model(model, test_gen, CLASSES)\n",
    "\n",
    "test_acc = evaluation_results['test_accuracy']\n",
    "test_loss = evaluation_results['test_loss']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìà RESULTADOS FINALES\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"Test Loss:     {test_loss:.4f}\")\n",
    "\n",
    "# Verificar si se alcanz√≥ el objetivo\n",
    "if test_acc >= 0.85:\n",
    "    print(f\"\\nüéâ ¬°OBJETIVO DE ACCURACY ALCANZADO! (>85%)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Accuracy por debajo del objetivo: {test_acc:.4f} < 0.85\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìã M√âTRICAS POR CLASE\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "recall_objetivo_alcanzado = True\n",
    "for class_name in CLASSES:\n",
    "    metrics = evaluation_results['classification_report'][class_name]\n",
    "    recall = metrics['recall']\n",
    "    precision = metrics['precision']\n",
    "    f1 = metrics['f1-score']\n",
    "    \n",
    "    status = \"‚úÖ\" if recall >= 0.80 else \"‚ùå\"\n",
    "    \n",
    "    print(f\"\\n{status} {class_name}:\")\n",
    "    print(f\"  Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "    print(f\"  Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "    print(f\"  F1-Score:  {f1:.4f} ({f1*100:.2f}%)\")\n",
    "    \n",
    "    if recall < 0.80:\n",
    "        recall_objetivo_alcanzado = False\n",
    "\n",
    "if recall_objetivo_alcanzado:\n",
    "    print(f\"\\nüéâ ¬°OBJETIVO DE RECALL ALCANZADO EN TODAS LAS CLASES! (>80%)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Algunas clases tienen recall < 80%\")\n",
    "\n",
    "print(f\"\\n{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar gr√°ficos y resultados\n",
    "print(\"üíæ Guardando resultados...\\n\")\n",
    "\n",
    "# 1. Gr√°fico de entrenamiento\n",
    "plot_path = LOGS_DIR / 'mobilenetv3_ultimate_training_history.png'\n",
    "plot_training_history(history, plot_path)\n",
    "print(f\"‚úÖ Gr√°fico guardado: {plot_path}\")\n",
    "\n",
    "# 2. Matriz de confusi√≥n\n",
    "cm_path = LOGS_DIR / 'mobilenetv3_ultimate_confusion_matrix.png'\n",
    "cm = plot_confusion_matrix(\n",
    "    evaluation_results['y_true'],\n",
    "    evaluation_results['y_pred'],\n",
    "    CLASSES,\n",
    "    cm_path\n",
    ")\n",
    "print(f\"‚úÖ Matriz de confusi√≥n guardada: {cm_path}\")\n",
    "\n",
    "# 3. Modelo final\n",
    "model_path = MODELS_DIR / 'mobilenetv3_ultimate_final.keras'\n",
    "model.save(str(model_path))\n",
    "print(f\"‚úÖ Modelo final guardado: {model_path}\")\n",
    "\n",
    "# 4. Log detallado\n",
    "hyperparameters = {\n",
    "    'model_name': 'MobileNetV3-Large ULTIMATE',\n",
    "    'architecture': 'Dense(384)->Dense(192) [100/100]',\n",
    "    'image_size': IMAGE_SIZE,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs_initial': EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'lr_schedule': 'CosineDecay',\n",
    "    'optimizer': 'Adam',\n",
    "    'dropout': [0.4, 0.35],\n",
    "    'l2_regularization': 0.001,\n",
    "    'fine_tuning_layers': 10,\n",
    "    'fine_tuning_lr': LEARNING_RATE * 0.025,\n",
    "    'fine_tuning_epochs': 10\n",
    "}\n",
    "\n",
    "log_path = LOGS_DIR / 'mobilenetv3_ultimate_training_log.json'\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "save_training_log(\n",
    "    log_path,\n",
    "    'MobileNetV3-Large ULTIMATE',\n",
    "    hyperparameters,\n",
    "    history,\n",
    "    evaluation_results,\n",
    "    cm,\n",
    "    total_time\n",
    ")\n",
    "print(f\"‚úÖ Log guardado: {log_path}\")\n",
    "\n",
    "# 5. Resumen final\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üéâ ¬°ENTRENAMIENTO COMPLETADO EXITOSAMENTE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"‚è±Ô∏è  Tiempo total: {total_time/60:.2f} minutos\")\n",
    "print(f\"üìä Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"üìä Objetivo Accuracy (>85%): {'‚úÖ ALCANZADO' if test_acc >= 0.85 else '‚ùå NO ALCANZADO'}\")\n",
    "print(f\"üìä Objetivo Recall (>80%): {'‚úÖ ALCANZADO' if recall_objetivo_alcanzado else '‚ùå NO ALCANZADO'}\")\n",
    "print(f\"\\nüíæ Todos los archivos guardados en:\")\n",
    "print(f\"   ‚Ä¢ Modelo: {model_path}\")\n",
    "print(f\"   ‚Ä¢ Logs: {LOGS_DIR}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
